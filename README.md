Este archivo debe ir en la carpeta ra√≠z `Sensor-fusion/` y sirve como portada del proyecto.

```markdown
# üöó Multi-Modal Sensor Fusion for Autonomous Driving

![Project Banner](assets/banner_demo.png)
*(Coloca aqu√≠ una imagen impactante que combine visi√≥n y datos)*

Repositorio integral para la percepci√≥n en conducci√≥n aut√≥noma. Este proyecto implementa pipelines de **Visi√≥n Computacional**, **Procesamiento Lidar/Radar** y **Fusi√≥n de Sensores** para la detecci√≥n robusta de objetos y carriles en entornos complejos (BDD100K, NuScenes).

## üåü Caracter√≠sticas Principales
* **Visi√≥n:** Comparativa SOTA (YOLO11, RT-DETR, YOLOP, PolyLaneNet).
* **Entrenamiento:** Scripts de *Fine-tuning* para adaptar modelos a datasets de conducci√≥n.
* **Benchmarks:** Herramientas automatizadas para medir mAP y Latencia.
* **Interfaz:** App interactiva basada en Streamlit.

## üõ†Ô∏è Instalaci√≥n

1.  **Clonar el repositorio:**
    ```bash
    git clone [https://github.com/jralbino/Sensor-fusion.git](https://github.com/jralbino/Sensor-fusion.git)
    cd Sensor-fusion
    ```

2.  **Configurar entorno virtual:**
    ```bash
    python -m venv venv
    
    # Windows
    venv\Scripts\activate
    # Linux/Mac
    source venv/bin/activate
    ```

3.  **Instalar dependencias:**
    ```bash
    pip install -r requirements.txt
    ```

## üì¶ Modelos Necesarios
Para que el proyecto funcione al 100%, descarga los siguientes pesos y col√≥calos en `Vision/models/`:

| Modelo | Descripci√≥n | Archivo |
|--------|-------------|---------|
| **YOLO11** | Detecci√≥n General | `yolo11l.pt`, `yolo11x.pt` |
| **RT-DETR** | Transformer (Original) | `rtdetr-l.pt` |
| **RT-DETR** | **Finetuned (Ours)** | `rtdetr-bdd-best.pt` |
| **UFLD** | Lane Detection R√°pida | `tusimple_18.pth` |
| **PolyLaneNet** | Regresi√≥n de Carriles | `model_2305.pt` |

## üöÄ Quick Start (Visi√≥n)

Para probar el m√≥dulo de visi√≥n inmediatamente:

```bash
cd Vision
streamlit run app.py